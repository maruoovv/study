## 하둡 분산 파일 시스템 

데이터가 단일 물리 머신의 저장 용량을 초과하게 되면 전체 데이터셋을 분리된 여러 머신에 나눠 저장할 필요가 있다.  
네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일 시스템을 **분산 파일시스템**이라 한다.  
분산 파일시스템은 네트워크 기반이므로 네트워크 프로그램의 복잡성을 모두 가지고 있다.  

하둡은 HDFS 라는 분산 파일 시스템을 제공한다.

### HDFS 설계 
HDFS의 설계 특징을 살펴보자.

- 매우 큰 파일 
	- '매우 큰' 의 의미는 수백 MB, GB, TB 크기의 파일을 의미한다. 최근에는 페타바이트 크기의 데이터를 저장하는 하둡 클러스터도 있다.
- 스트리밍 방식의 데이터 접근 
	- HDFS는 '가장 효율적인 데이터 처리 패턴은 한 번 쓰고 여러 번 읽는 것' 이라는 아이디어에서 출발했다. 데이터셋은 생성되거나 원본으로부터 복사된다.
- 범용 하드웨어 
	- 하둡은 고가의 신뢰도 높은 하드웨어만을 고집하지 않는다. 노드 장애가 발생할 확률이 높은 범용 하드웨어로 구성된 대형 클러스터에서 문제없이 실행되도록 설계되었다.
	
HDFS 가 적합하지 않은 분야는 다음과 같다..

- 빠른 데이터 응답 시간 
	- 데이터 접근에 수십 밀리초 수준의 빠른 응답 시간을 요구하는 애플리케이션은 맞지 않다. 
	HDFS는 높은 데이터 처리량을 제공하기 위해 응답 시간을 희생했다. 빠른 응답 시간을 원한다면 HBase가 하나의 대안이 될 수 있다. 

- 수많은 작은 파일 
	- 네임노드는 파일시스템의 메타데이터를 메모리에서 관리하기 때문에 저장할 수 있는 파일 수는 네임노드의 메모리 용량에 좌우된다.
	파일수가 백만개이고 각 파일의 블록수가 하나라고 한다면 최소 300MB 의 메모리가 필요하다. 파일 수가 많아지면 하드웨어 용량을 넘어서게 될 수도 있다.
	
- 다중 라이터와 파일의 임의 수정
	- HDFS 는 단일 라이터로 파일을 쓴다. 파일의 임의 위치에 있는 내용을 수정하는 것은 허용하지 않으며 다중 라이터도 지원하지 않는다. (하둡 3.0 부터 지원)
	
### HDFS 개념

#### 블록 

물리적인 디스크는 블록 크기란 개념이 있다. 블록 크기는 한 번에 읽고 쓸 수 있는 데이터의 최대량이다.  
파일시스템 블록의 크기는 보통 수 킬로 바이트이고, 디스크 블록의 크기는 기본적으로 512 바이트이다.
    
HDFS 도 블록의 개념을 갖고 있다. HDFS 블록은 기본적으로 128MB 와 같이 굉장히 큰 단위다.  
HDFS 의 파일은 특정 블록 크기의 청크로 쪼개지고, 각 청크는 독립적으로 저장된다.  
단일 디스크를 위한 파일시스템은 디스크 블록 크기보다 작은 데이터라도 한 블록 전체를 점유하지만,  
HDFS 파일은 블록 크기보다 작은 데이터일 경우 전체 블록을 점유 하지는 않는다.  
예를들어 HDFS의 블록 크기가 128MB 이고, 1MB 의 파일을 저장한다면 1MB의 디스크만 사용한다.  


분산 파일시스템에 블록 개념을 도입하면서 얻게 된 이득이 있다.

- 파일 하나의 크기가 단일 디스크의 용량보다 더 커질 수 있다.
	- 하나의 파일을 구성하는 여러 개의 블록이 동일한 디스크에만 저장될 필요가 없으므로, 클러스터에 있는 어떤 디스크에도 저장될 수 있다. 
- 스토리지의 서브시스템을 단순하게 만들 수 있다.
	- 분산 시스템은 장애 유형이 너무나도 다양하기 때문에, 시스템을 단순화 하는게 매우 중요하다.
- 복제를 구현하기에 적합하다.
	- 내고장성과 가용성을 제공하는데 필요한 복제를 구현하기가 적합하다.
	- 각 블록은 블록의 손상과 디스크 장애에 대처하기 위해 물리적으로 분산된 다수에 머신에 복제된다. 만일 한 블록이 장애라면, 다른 머신의 복사본을 읽도록 하게 하면 된다.

#### 네임노드와 데이터노드

HDFS 는 마스터-워커 패턴으로 동작하는 네임노드(마스터)와 데이터노드(워커)로 구성된다.  

##### 네임노드

파일시스템의 네임스페이스를 관리한다.  
파일시스템 트리와 그 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터를 유지한다.  
파일에 속한 모든 블록이 어떤 데이터노드에 있는지 파악하고 있다.
네임노드를 실행하는 머신이 손상되면 파일시스템의 어떤 파일도 찾을 수 없다.  
따라서 네임노드의 장애복구 기능은 필수적이며, 하둡은 두가지 매커니즘을 제공한다.

1. 파일시스템의 메타데이터를 지속적인 상태로 보존하기 위해 파일로 백업.
2. 보조네임노드를 운영.
	- 주 네임노드와는 조금 다르게 동작한다. 
	- 주 네임노드가 남긴 에디트 로그가 너무 커지지 않도록 주기적으로 네임스페이스 이미지와 병합하여 새로운 네임스페이스 이미지를 만든다.
	- 네임스페이스 이미지의 복제본을 보관한다.
	- 하지만 네임스페이스 이미지는 약간의 시차를 두고 복제되기 때문에 어느 정도의 데이터 손실은 불가피하다.

##### 데이터노드
클라이언트나 네임노드의 요청이 있을 때 블록을 저장하고 탐색하며, 저장하고 있는 블록의 목록을 주기적으로 네임노드에 보낸다.  

#### 블록 캐싱 
데이터노드는 빈번하게 접근하는 블록 파일을 오프힙 블록 캐시라는 데이터노드의 메모리에 명시적으로 캐싱할 수 있다.   
잡 스케줄러(맵리듀스, 스파크 등)은 블록이 캐싱된 데이터노드에서 태스크가 실행되도록 할 수 있으며, 이러한 블록 캐시의 장점을 이용해 읽기 성능을 높일 수 있다.  

#### HDFS 페더레이션 
네임노드는 파일시스템의 모든 파일과 각 블록의 참조 정보를 메모리에서 관리한다.  
따라서 파일이 매우 많은 대형 클러스터에서 확장의 걸림돌은 메모리이다.  
네임노드의 확장성 문제를 해결하기 위해 하둡은 HDFS Federation(연합체)를 지원한다.  
각각의 네임노드가 네임스페이스 일부를 나누어 관리하는 방식으로 새로운 네임노드를 추가할 수 있다.

```
ex) 
/user 디렉토리 아래 파일 -> namenode1
/share 디렉토리 아래 파일 -> namenode2
```

HDFS federation 을 적용하면 각 네임노드는 네임스페이스와 메타데이터를 구성하는 **네임스페이스 볼륨** 과 네임스페이스에 포함된 파일의 전체 블록을 보관하는 **블록풀**을 관리한다.  
네임스페이스 볼륨은 서로 독립되어 있어, 네임노드끼리 통신할 필요가 없고 특정 네임노드에 장애가 발생해도 다른 네임스페이스의 가용성에 영향을 주진 않는다.  
하지만 블록 풀의 저장소는 분리되어 있지 않아 모든 데이터 노드는 각 네임노드마다 등록되어 있다.

#### HDFS 고가용성 
하둡이 네임노드에 대해 장애복구 기능을 제공한다고 해도 고가용성을 보장해주지는 않는다.  
네임노드는 여전히 SPOF 이다.  
네임노드에 장애가 발생하면 맵리듀스 잡을 포함해 모든 클라이언트가 읽거나 쓰거나 조회할 수 없다.
장애 복구 기능이 존재하지만, 대형 클러스터에서 네임노드를 재구동 하는 경우 30분 이상 걸리는 경우도 있다.  

이 문제를 해결하기 위해 하둡 2.x 부터 HDFS High Availability(HA) 를 지원한다.  
HA는 active-standby 상태로 설정된 한 쌍의 네임 노드로 구현된다.  
active namenode 에 장애가 발생하면 standby namenode 가 역할을 이어 받아 큰 중단 없이 요청을 처리한다.  

##### 장애복구와 펜싱
대기 네임노드를 활성화시키는 전환 작업은 장애복구 컨트롤러 라는 새로운 객체로 관리된다.  
각 네임노드는 경량의 장애복구 컨트롤러 프로세스로 네임노드의 장애를 감시(하트비트 방식 사용)하고 장애가 발생하면 복구를 지시한다.  
장애가 발생 했을 때, 기존의 액티브 네임노드가 시스템을 손상시키지 않도록 해야하는데, 이를 위해 펜싱이라는 메서드를 제공한다.  
(standby -> active 로 전환 될 때, 기존의 active 를 확실히 죽임)

![image](https://user-images.githubusercontent.com/37106689/76208234-9d05fb80-6242-11ea-8963-63819fe871a8.png)

### 명령행 인터페이스 

명령행 인터페이스 실습 전에 macOS에 hadoop 을 설치해보자  
https://tariat.tistory.com/492 참고  
위 링크가 잘 안되어서 설정은 책의 부록으로 함.

### 하둡 파일시스템

하둡의 파일시스템은 추상화 개념을 가지고 있고, HDFS 는 구현체 중 하나일 뿐이다.  
하둡은 다양한 파일시스템을 위한 인터페이스를 제공하는데, 접근할 파일시스템의 인스턴스를 선택할때 일반적으로 URI 스킴을 사용한다.  

#### 인터페이스 
하둡은 자바로 작성되었기 때문에 자바 API를 통해 하둡 파일시스템과 연동할 수 있다.  
파일시스템 인터페이스를 JAVA API 뿐 아니라 다양한 방법으로 지원한다.
필요할 때 찾아보면 될듯.

### 데이터 흐름

#### 파일 읽기 흐름
클라이언트가 HDFS, 네임노드, 데이터노드와 어떻게 상호작용하는지 살펴보자.

![image](https://user-images.githubusercontent.com/37106689/76314911-048f7a00-631b-11ea-9345-9f710c21c421.png)

1. HDFS 클라이언트는 DistributedFileSystem 인스턴스인 FileSystem 객체의 open() 메서드를 호출해 원하는 파일을 연다.
2. DFS 는 파일의 첫 번쨰 블록 위치를 파악하기 위해 네임노드를 호출한다. 네임노드는 블록별로 해당 블록의 복제본을 가진 데이터노드의 주소를 반환한다.  
이 때 클러스터의 네트워크 위상에 따라 가까운 순으로 데이터노드가 정렬된다.  
DFS 는 클라이언트가 데이터를 읽을 수 있도록 FSDataInputStream 을 반환한다. 
3. 클라이언트는 스트림을 읽기 위해 read() 메서드를 호출한다. 파일의 첫 번째 블록의 데이터노드와 연결한다. 
4. 메서드를 반복적으로 호출해 한 블록을 다 읽으면 
5. 다음 블록의 데이터노드를 찾는다. 블록마다 블록의 데이터노드 위치를 얻기 위해 네임노드를 호출하고 데이터노드와 연결을 맺는다.
6. 모든 블록을 다 읽으면 close() 를 호출한다.

데이터를 읽는 도중에 데이터노드와 통신의 장애가 발생하면 해당 블록을 저장한 다른 데이터노드와 연결을 시도한다.  
이후 장애가 발생한 블록에 대해 불필요한 재시도를 방지하기 위해 장애를 기억해둔다.

#### 파일 쓰기 흐름
HDFS 에 파일을 쓰는 방법을 살펴보자.

![image](https://user-images.githubusercontent.com/37106689/76315677-87650480-631c-11ea-892b-673e40eabf93.png)

1. 클라이언트는 DFS의 create() 를 호출하여 파일을 생성한다.
2. 파일시스템 네임스페이스에 새로운 파일을 생성하기 위해 네임노드에 요청을 보낸다.  
네임노드는 요청한 파일과 동일한 파일이 존재하는지, 권한이 있는지 등 검사를 수행한다.
3. 클라이언트가 데이터를 쓸 때, DFSOutputStream 은 데이터를 패킷으로 분리하고, **데이터 큐** 라 불리는 내부 큐로 패킷을 보낸다.  
DataStreamer 는 데이터 큐의 패킷을 처리한다. 
4.
	1) 먼저 네임노드에 복제본을 저장할 데이터노드 목록 요청
	2) 데이터노드들은 파이프 라인을 형성하는데, 복제 수준이 3이면 세개의 노드가 파이프라인에 속함
	3) DS 는 파이프라인의 첫 번째 노드로 패킷을 전송한다.
	4) 첫 번째 노드는 각 패킷을 저장하고 파이프라인의 다음 노드로 보낸다..
	5) 두번째 노드->세번째도 동일
5. DFSOutputStream 은 데이터노드의 승인 여부를 기다리는 ack큐를 유지한다. ack큐 패킷은 파이프라인의 모든 데이터 노드로부터 ack 패킷을 받아야 제거된다.  
데이터를 쓰는 중에 데이터노드에 장애가 발생하면, ack 큐의 모든 패킷은 다시 데이터 큐의 앞쪽에 추가되고, 새로운 파이프라인을 구성한다.
