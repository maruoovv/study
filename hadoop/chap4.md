## YARN
YARN 은 핟부의 클러스터 자원 관리 시스템이다.  
맵리듀스의 성능을 높이기 위해 하둡2에서 처음 도입되었다.  
YARN 은 맵릳스 뿐 아니라 다른 분산 컴퓨팅 도구도 지원한다.

YARN은 클러스터 자원을 요청하고 사용하기 위한 API를 제공한다.  
사용자는 이를 YARN이 내장된 분산 컴퓨팅 프레임워크에서 고수준 API를 사용하며, 자원 관리의 자세한 내용은 알 수 없다.

![image](https://user-images.githubusercontent.com/37106689/76621513-e0c67100-6572-11ea-8255-c08826fdf3d1.png)

맵리듀스, 스파크 등 분산 컴퓨팅 프레임워크는 클러스터 계산 계층(YARN)과 클러스터 저장 계층(HDFS, Hbase) 위에서 YARN 애플리케이션을 실행한다.


### YARN 애플리케이션 수행 해부해보기
YARN은 리소스 매니저와 노드 매니저 등 두가지 유형의 장기 실행 데몬을 통해 핵심 서비스를 제공한다.  
**리소스 매니저** 는 클러스터 전체 자원의 사용량을 관리하고, 모든 머신에서 실행되는 **노드 매니저**는 **컨테이너**를 구동하고 모니터링하는 역할을 맡는다.  

![image](https://user-images.githubusercontent.com/37106689/76622803-94c8fb80-6575-11ea-917c-9e8e4abd4b97.png)

1. 클라이언트는 YARN에서 애플리케이션을 구동하기 위해 리소스 매니저에 접속하여 **애플리케이션 마스터** 프로세스의 구동을 요청한다.
2. 리소스 매니저는 컨테이너에서 애플리케이션 마스터를 시작할 수 있는 노드 매니저를 하나 찾는다.
3. 애플리케이션 마스터는 단일 컨테이너에서 계산을 수행하고 결과를 반환 후 종료하거나, 리소스 매니저에 더 많은 컨테이너를 요청한 후 분산 처리를 수행 하는 경우도 있다.

#### 자원 요청 
분산 데이터 처리 알고리즘에서 클러스터의 네트워크 대역폭을 효율적으로 활용하기 위해서는 지역성을 보장하는 것이 가장 중요하다.  
지역성 제약이 불가능할 때가 있는데, 이때는 할당이 실패하거나 또는 제약을 조금 느슨하게 적용할 수 있다.  

#### 애플리케이션의 수명
YARN 애플리케이션의 수명은 몇 초 만에 끝나는 짧은 수명부터, 며칠 또는 몇달이 걸리는 긴 수명까지 차이가 크다.  
실행 시간보다는 실행하는 잡의 방식에 따라 애플리케이션을 분류하는 것이 좋다.

#### YARN 과 맵리듀스 1의 차이
하둡 구버전의 맵리듀스 분산 구현은 '맵리듀스1'로, YARN 을 이용한 구현은 '맵리듀스2'로 구분한다.  

맵리듀스1은 잡의 실행 과정을 제어하는 하나의 **잡트래커**와 하나 이상의 **태스크트래커** 등 두 종류의 데몬이 있다.  
잡트래커는 여러 태스크트래커에서 실행되는 태스크를 스케줄링 한다.  
태스크트래커는 태스크를 실행하고 진행 상황을 잡트래커에 전송하기 때문에 잡트래커는 각 잡의 전체적인 진행 상황을 파악할 수 있다.  

맵리듀스1 에서 잡트래커는 잡 스케줄링과 태스트 진행 모니터링을 맡고 있다.  
반면 YARN 은 이러한 역할을 분리된 객체인 리소스 매니저와 애플리케이션 마스터를 통해 처리한다.  

|MapReduce1|YARN|
|---|---|
|잡트래커|리소스 매니저, 애플리케이션 마스터, 타임라인 서버|
|태스크트래커|노드 매니저|
|슬롯|컨테이너|

YARN 은 맵리듀스1의 여러 한계를 극복하기 위해 설계되었다. YARN 의 장점은 다음과 같다.

- 확장성  
	YARN 은 맵리듀스1보다 큰 클러스터에서 실행될 수 있다. 맵리듀스1은 4000노드나 40000태스크를 넘어서면 병목현상이 발생한다.  
	잡트래커가 잡과 태스크를 모두 관리하기 떄문이다.  
	YARN 은 리소스 매니저와 애플리케이션 마스터를 분리한 구조로 이러한 한계를 극복했다. 10000노드, 100000 태스크 까지 확장할 수 있다.

- 가용성
	맵리듀스1에서는 잡트래커 서비스에 HA를 적용하는 것이 어려웠다.  
	YARN 에서는 잡트래커의 역할이 분리되었기 때문에 HA 서비스가 분할 후 정복 문제로 바뀌었다.

- 효율성
	맵리듀스1은 각 태스크틀커는 맵 슬롯과 리슈스 슬롯으로 구분되어 있다.  
	맵 슬롯은 맵 태스트 실행에만 사용할 수 있고 리듀스 슬롯은 리듀스 태스크에만 사용할 수 있다.  
	YARN의 노드 매니저는 정해진 개수의 슬롯 대신 리소스 풀을 관리한다.  
	YARN은 맵리듀스1에서 발생하는, 맵 슬롯은 남아 있지만 리듀스 슬롯이 없어 리듀스 태스크가 계속 대기하는 상황은 발생하지 않는다.  

### YARN 스케줄링 
현실 세계에서는 자원이 제한되어 있어 어떤 애플리케이션은 요청이 처리될 때까지 기다려야 한다.  
YARN 스케줄러의 역할은 정해진 정책에 따라 애플리케이션에 자원을 할당한다.

#### 스케줄러 옵션
YARN은 FIFO, Capacity, Fair 스케줄러를 제공한다.

##### FIFO
애플리케이션을 큐에 하나씩 넣고 제출된 순서에 따라 순차적으로 실행한다.  
FIFO는 이해하기 쉽고 별다른 설정이 필요 없다는 장점이 있지만, 공유 클러스터 환경에서는 적합하지 않다.   
대형 애플리케잇녀이 수행될 때는 클러스터의 모든 자원을 점유할 수 있기 때문에 다른 애플리케이션은 계속 대기해야 한다.  

##### Capacity
캐퍼시티 스케줄러는 작은 잡을 제출되는 즉시 분리된 전용 큐에서 처리한다.  
해당 큐는 잡을 위한 자원을 미리 예약해두기 때문에 전체 클러스터의 효율성은 떨어진다.  
회사의 조직 체계에 맞게 하둡 클러스터를 공유할 수 있다.  
각 단일 큐 내부의 애플리케이션은 FIFO 방식으로 스케줄링 된다.  
하나의 단일 잡은 해당 큐의 가용량을 넘는 자원은 사용할 수 없다.  
그러나 큐 안에 다수의 잡이 존재하고 가용 자원이 남아 있다면, 해당 큐의 잡을 위해 여분의 자원을 할당하여 큐의 가용량을 초과하게 된다.  
이러한 방식을 **큐 탄력성** 이라 한다.

##### Fair
페어 스케줄러는 실행 중인 모든 잡의 자원을 공평하게 동적으로 분배한다.  
대형 잡이 먼저 실행되면 잡이 하나밖에 없기 때문에 클러스터의 모든 자원을 얻는다.  
대형 잡이 실행되는 도중에 작은 잡이 하나 추가되면 클러스터의 자원 절반을 이 잡에 할당한다.  


#### 지연 스케줄링
YARN의 모든 스케줄러는 지역성 요청을 가장 우선시 한다.  
바쁜 클러스터에 어떤 애플리케이션 요청이 들어 왔을 때, 그 노드에 다른 컨테이너가 실행되고 있을 가능성이 높다.  
실제로 조금만 기다리면 이 요청한 노드에서 컨테이너를 할당 받을 수 있는 기회가 증가하는데, 이는 클러스터의 효율성도 높아지게 되는 결과를 가져온다.  
이를 위해 **지연 스케줄링** 이란 기능이 있으며, capacity, fair 스케줄러는 모두 이러한 기능을 제공한다.  
각 노드 매니저는 주기적으로 리소스 매니저에 하트비트를 보내는데, 실행 중인 컨테이너의 정보와 새로운 컨테이너를 위한 가용 자원에 대한 정보를 주고 받는다.  
각 하트비트는 애플리케이션이 실행할 컨테이너를 얻을 수 있는 중요한 **스케줄링 기회**가 된다.  

지연 스케줄링을 사용할 때 스케줄러는 처음 오는 스케줄링 기회를 바로 잡지 않고, 허용한 스케줄링 기회의 최대 횟수까지 기다린 후 잡는다.  

#### 우성 자원 공평성 
메모리와 같은 단일 유형의 자원을 배분할 때는 가용량이나 공평성의 개념을 결정하는 것은 어렵지 않다.  
두 애플리케이션의 메모리양을 기준으로 비교하면 된다.  
하지만 다수의 자원이 있으면 복잡해진다.  
YARN 의 스케줄러가 이러한 문제를 처리하는 방법은 각 사용자의 우세한 자원을 확인한 후 이를 클러스터 사용량의 측정 기준으로 삼는 것이다.  
이러한 접근 방법을 **우성 자원 공평성(Dominant Resource Fairness)** 라 부른다.

ex) 클러스터 전체 CPU 100, 메모리 10TB

||A|B|
|---|---|---|
|CPU|2|6|
|메모리|300|100|

위의 예에서, A는 CPU 2개와 300GB 메모리, B는 CPU 6개와 100GB 메모리를 요청했다.  
A의 요청은 전체 클러스터의 CPU: 2% MEMORY: 3%,  B는 CPU : 6%, MEMORY : 1% 이다.  
각 앱의 우세 자원은 A는 메모리 비율이 3%로 더 크고, B는 CPU가 6%로 더 크다.  
이를 A와 B를 비교 하면 B가 A보다 두배가 높다. 따라서 컨테이너 A는 B의 절반에 해당하는 자원을 할당 받게 될 것이다.  
이 기능은 기본적으로 비활성화 되어 있어, 자원을 계산할 때 CPU는 무시하고 메모리만 고려된다.
