검색엔진으로 ES 를 사용중인데, 특정 한자가 검색이 안된다는 이슈가 있어 분석 해봤다.

검색이 안된다고 문의가 들어왔던건 '李' 한자였다.

### ES character filter

엘라스틱 서치는 character filter 라는걸 사용해서 입력값이 토크나이저에 들어가기 전에 필터링 하거나 변환 할수 있다.

ES에 기본 정의된 필터들도 있고, 커스텀하게 만들어서 사용할 수도 있다.

[https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-charfilters.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-charfilters.html)

현 검색시스템에는 여러가지 필터들이 있는데, 영어 대/소문자 변환, 공백제거, 허용된 문자만 필터링 (숫자, 영문, 한글, 한자..) 등의 필터가 설정되어 있었다.

이중 허용된 문자만 필터링 하는 필터가 존재했는데, 해당 필터에선 한자 범위 캐릭터를 허용하고 있었다.

```
"test_filter": {
  "pattern": "(^'+)|('+$)|(\\s+'+)|('+\\s+)|[^%&＆\\+'\\.,°#\\(\\)\\-\\~/0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣|一-龥]",
  "type": "pattern_replace",
  "replacement": " "
},
```

분명 한자를 허용하고 있는데, 왜 검색이 되지 않는걸까?

테스트를 위해 es 에 filter 와 analyzer 를 생성하고 테스트를 해보자

```
PUT /test_index/_settings
{
    "index": {
        "analysis": {
            "char_filter": {
                "test_filter": {
                    "pattern": "(^'+)|('+$)|(\\s+'+)|('+\\s+)|[^%&＆\\+'\\.,°#\\(\\)\\-\\~/0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣|一-龥]",
                    "type": "pattern_replace",
                    "replacement": " "
                }
            }
        }
    }
}

{
    "index": {
        "analysis": {
                "analyzer": {
                    "test_analyzer": {
                        "filter": [
                        ],
                        "char_filter": [
                            "test_filter"
                        ],
                        "type": "custom",
                        "tokenizer": "standard"
                    }
                }

        }
    }
}
```

```
/test_index/_analyze
{
    "analyzer": "test_analyzer",
    "explain": true,
    "text": "李 車 李 車"
}
```

결과

```
{
    "detail": {
        "custom_analyzer": true,
        "charfilters": [
            {
                "name": "test_filter",
                "filtered_text": [
                    "李 車    "
                ]
            }
        ],
        "tokenizer": {
            "name": "standard",
            "tokens": [
                {
                    "token": "李",
                    "start_offset": 0,
                    "end_offset": 1,
                    "type": "<IDEOGRAPHIC>",
                    "position": 0,
                    "bytes": "[e6 9d 8e]",
                    "positionLength": 1,
                    "termFrequency": 1
                },
                {
                    "token": "車",
                    "start_offset": 2,
                    "end_offset": 3,
                    "type": "<IDEOGRAPHIC>",
                    "position": 1,
                    "bytes": "[e8 bb 8a]",
                    "positionLength": 1,
                    "termFrequency": 1
                }
            ]
        },
        "tokenfilters": []
    }
}
```

분명 한자 범위 문자를 허용하고 있는데, 필터 결과와 분석 결과에는 앞의 두 문자만 나오고 뒤의 문자에는 나오지 않는다.

filter 부분이 아닌 anaylzer, tokenizer 부분들을 더 살펴보았지만 특별한건 발견할수 없었다.

그러다가 해당 한자를 유니코드 사전에 검색을 해보았고 해당 한자가 '한중일 호환용 한자' 라는걸 발견했다.

## 한중일 호환용 한자

[https://ko.wikipedia.org/wiki/한중일\_호환용\_한자](https://ko.wikipedia.org/wiki/%ED%95%9C%EC%A4%91%EC%9D%BC_%ED%98%B8%ED%99%98%EC%9A%A9_%ED%95%9C%EC%9E%90)

보통 우리가 사용하는 한자는 한중일 통합용 한자인데, 같은 한자이지만 다르게 발음되는 경우가 있다.

대표적인 경우가 위 위키에 있는 樂 한자.

이를 표현하기 위해 별도의 유니코드를 사용하고, 이와 같은 한자들을 모아 한중일 호환용 한자 집합을 만들었다.

우리가 위에 설정한 필터의 한자 범위는 한중일 통합용 한자 범위였기 때문에, 호환용 한자는 필터를 통과하지 못해 제거된 것이다.

## 맵핑 필터 추가

한중일 호환용 한자를 통합용 한자로 변환하기 위해, mapping filter 를 추가해보자

/usr/share/elasticsearch/config/dict 에 맵핑 파일 추가

```
\uF9E1=>\u674E (李)
```

filter / analyzer 변경

```
PUT /test_index/_settings
{
    "index": {
        "analysis": {
            "char_filter": {
                "chinese_filter": {
                    "type": "mapping",
                    "mappings_path": "dict/chinese.txt"
                }
            }
        }
    }
}

{
    "index": {
        "analysis": {
                "analyzer": {
                    "test_analyzer": {
                        "filter": [
                        ],
                        "char_filter": [
                            "chinese_filter",
                            "test_filter"
                        ],
                        "type": "custom",
                        "tokenizer": "standard"
                    }
                }

        }
    }
}
```

결과

```
{
    "detail": {
        "custom_analyzer": true,
        "charfilters": [
            {
                "name": "chinese_filter",
                "filtered_text": [
                    "李 車 李 車"
                ]
            },
            {
                "name": "test_filter",
                "filtered_text": [
                    "李 車 李  "
                ]
            }
        ],
        "tokenizer": {
            "name": "standard",
            "tokens": [
                {
                    "token": "李",
                    "start_offset": 0,
                    "end_offset": 1,
                    "type": "<IDEOGRAPHIC>",
                    "position": 0,
                    "bytes": "[e6 9d 8e]",
                    "positionLength": 1,
                    "termFrequency": 1
                },
                {
                    "token": "車",
                    "start_offset": 2,
                    "end_offset": 3,
                    "type": "<IDEOGRAPHIC>",
                    "position": 1,
                    "bytes": "[e8 bb 8a]",
                    "positionLength": 1,
                    "termFrequency": 1
                },
                {
                    "token": "李",
                    "start_offset": 4,
                    "end_offset": 5,
                    "type": "<IDEOGRAPHIC>",
                    "position": 2,
                    "bytes": "[e6 9d 8e]",
                    "positionLength": 1,
                    "termFrequency": 1
                }
            ]
        },
        "tokenfilters": []
    }
}
```

해당 호환용 한자를 맵핑 필터를 이용해 바꿔주고, 분석이 잘 된걸 확인할수 있었다!
